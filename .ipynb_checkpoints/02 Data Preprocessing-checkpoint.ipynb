{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "- Use the parameter 'usecols' to select all columns from the raw data that are needed\n",
    "- Use the parameter 'parse_dates' to have Pandas automatically parse date info as it is brought in\n",
    "- Use the paremeter 'index_col' to set the index to the datetime column if this is time series data\n",
    "- Use the .query() function to import data that's conditional upon another columns values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully imported.\n",
      "Data Shape: (20000, 379)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_data = pd.read_csv('./data/TrainingSet.csv')\n",
    "if isinstance(raw_data, pd.DataFrame):\n",
    "    print(\"Data successfully imported.\")\n",
    "else:\n",
    "    print(\"Data failed to import.\")\n",
    "\n",
    "\n",
    "test_set = pd.read_csv('./data/TestSet.csv')\n",
    "if isinstance(test_set, pd.DataFrame):\n",
    "    print(\"Test data successfully imported.\")\n",
    "else:\n",
    "    print(\"Test data failed to import.\")\n",
    "    \n",
    "# Remove unneeded columns\n",
    "del raw_data['timestamp']\n",
    "del test_set['timestamp']\n",
    "\n",
    "# Time series example\n",
    "# hourly_weather_data = pd.read_csv('./data/raw_weather_data.csv', usecols=['DATE','REPORT_TYPE','HourlyDryBulbTemperature', 'HourlyPrecipitation'] , parse_dates=[\"DATE\"], index_col=\"DATE\").query(\"REPORT_TYPE == 'FM-15'\")\n",
    "\n",
    "print(\"Data Shape:\",raw_data.shape) \n",
    "print(\"Test Shape:\",test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Data into Training, Validation, Test, and Target Sets\n",
    "##### It is very important that this step is done prior to data imputation, normalization, one hot encoding or other preprocessing steps.\n",
    "\n",
    "You should NEVER do anything which leaks information about your testing data BEFORE a split.  If you normalize before the split, then you will use the testing data to calculate the range or distribution of this data which leaks this information also into the training data and vice versa which \"contaminates\" your data and will lead to over-optimistic performance estimations on your testing data. This is true for all data preprocessing steps which change data based on all data points including also feature selection.\n",
    "\n",
    "What you SHOULD do instead is to create the normalization only on the training data and use the preprocessing model coming out of the normalization operator.  This preprocessing model can then be applied like any other model on the testing data as well and will change the testing data based on the training data (which is ok) but not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (16000, 378)\n",
      "Test Set Shape: (4000, 378)\n",
      "7063     2839.728205\n",
      "4309     2422.636497\n",
      "8102     2276.712858\n",
      "16617    2498.442852\n",
      "18398    2487.473308\n",
      "Name: job_performance, dtype: float64\n",
      "6173     2540.483752\n",
      "13687    2751.641890\n",
      "16952    3156.473677\n",
      "8445     2727.079721\n",
      "14974    3783.485098\n",
      "Name: job_performance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate data into training, validation, and test sets\n",
    "train_set, validation_set, train_targets, validation_targets = train_test_split(raw_data, raw_data['job_performance'], test_size=0.2)\n",
    "\n",
    "# Set target and drop from training/test set data\n",
    "del train_set['job_performance']\n",
    "del validation_set['job_performance']\n",
    "del test_set['job_performance']\n",
    "\n",
    "print(\"Training Set Shape:\",train_set.shape)\n",
    "print(\"Validation Set Shape:\",validation_set.shape)\n",
    "print(\"Test Set Shape:\",test_set.shape)\n",
    "print(train_targets.head(5))\n",
    "print(validation_targets.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "---\n",
    "- <strong>Missing Value Ratio Filter:</strong> Used to drop features that have more than a certain percentage of their rows empty\n",
    "- <strong>Imputation:</strong> The process of deciding how to fill the empty rows. This can be done by using the mean, median, or mode for numerical data or a constant for categorical data. There are also algorithms and machine learning libraries built solely for imputation that can be used.\n",
    "- <strong>Normalization / Standarization:</strong> Used to scale and center data. Which one to use depends on the dimensionality reduction techniques to be used.  \n",
    "- <strong>Low Variance Filter:</strong> This can be used on numerical data to remove features that are constants or others with very low variance.\n",
    "- <strong>One Hot Encoding</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_functions\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from pipeline_functions import Print, MissingValueRatioFilter, StartTimer, ForceToNumerical, \\\n",
    "ConvertToDataFrame, HighCorrelationFilter, OutputRunTime, ChangeDType\n",
    "\n",
    "X = train_set.copy(deep=True)\n",
    "V = validation_set.copy(deep=True)\n",
    "T = test_set.copy(deep=True)\n",
    "\n",
    "# Numerical transformations\n",
    "numerical_missing_ratio = 0.5\n",
    "variance_threshold = 0.01\n",
    "numerical_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  \n",
    "numerical_features = list(X.select_dtypes(include=numerical_colums).columns)  \n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('print1', Print(message=\"  Preprocessing:\")),\n",
    "    ('print2', Print(message=\"    Numerical: Missing Value Ratio Filter (>\"+str(numerical_missing_ratio)+\")\")),\n",
    "    ('print3', Print(message=\"      Starting Numerical Features: \",columns=True)),\n",
    "    ('missing_value_ratio_filter', MissingValueRatioFilter(ratio_missing=numerical_missing_ratio)),\n",
    "    ('print4', Print(message=\"      Remaining Numerical Features:\",columns=True)),\n",
    "    ('print5', Print(message=\"    Numerical: Imputation\")),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('print6', Print(message=\"    Numerical: Normalization\")),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('print7', Print(message=\"    Numerical: Low Variance Filter (>\"+str(variance_threshold)+\")\")),\n",
    "    ('print8', Print(message=\"      Starting Numerical Features: \",columns=True)),\n",
    "    ('variance_threshold', VarianceThreshold(threshold=variance_threshold)),\n",
    "    ('print9', Print(message=\"      Remaining Numerical Features:\",columns=True))\n",
    "    ])\n",
    "\n",
    "# Categorical transformations\n",
    "categorical_missing_ratio = 0.5\n",
    "categorical_variance_threshold = 0.01\n",
    "categorical_features = X.select_dtypes(['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('print0', Print(message=\"    Categorical: Missing Value Ratio Filter (>\"+str(categorical_missing_ratio)+\")\")),\n",
    "    ('print1', Print(message=\"      Starting Categorical Features: \",columns=True)),\n",
    "#     ('missing_value_ratio_filter', MissingValueRatioFilter(ratio_missing=categorical_missing_ratio)),\n",
    "    ('print2', Print(message=\"      Remaining Categorical Features:\",columns=True)),\n",
    "    ('print3', Print(message=\"    Categorical: Imputation\")),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('print4', Print(message=\"    Categorical: Conversion of Ints to Strings\")),\n",
    "    ('change_dtype', ChangeDType()),\n",
    "    ('print5', Print(message=\"    Categorical: One Hot Encoding\")),\n",
    "    ('print6', Print(message=\"      Starting Categorical Features:\",columns=True)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('print7', Print(message=\"      Remaining Categorical Features:\",columns=True)),\n",
    "    ('print8', Print(message=\"    Categorical: Low Variance Filter (>\"+str(categorical_variance_threshold)+\")\")),\n",
    "    ('print9', Print(message=\"      Starting Categorical Features: \",columns=True)),\n",
    "#     ('variance_threshold', VarianceThreshold(threshold=categorical_variance_threshold)),\n",
    "    ('print10', Print(message=\"      Remaining Categorical Features:\",columns=True))\n",
    "    ])\n",
    "\n",
    "# Combine numerical and categorical data back together\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Master pipeline\n",
    "high_correlation_filter_decimal = 0.9\n",
    "master_pipeline = Pipeline([\n",
    "    ('start_timer', StartTimer()),\n",
    "    ('print1', Print(message=\"\\nStarting Shape: \" + str(X.shape))),\n",
    "    ('print2', Print(message=\"  Forcing column 'v71' to numerical data.\")),\n",
    "    ('force_to_numerical', ForceToNumerical()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('print3', Print(message=\"  Recombined Numerical & Categorical Shape: \",return_shape=True)),\n",
    "    ('print4', Print(message=\"  Dimensionality Reduction: \")),\n",
    "    ('convert_to_dataframe', ConvertToDataFrame()),\n",
    "    ('print5', Print(message=\"    High Correlation Filter (> \" + str(high_correlation_filter_decimal) + \")\")),\n",
    "    ('high_correlation_filter', HighCorrelationFilter(correlation_decimal=high_correlation_filter_decimal)),\n",
    "    ('print6', Print(message=\"Final Shape:\",return_shape=True)),\n",
    "#     ('output_run_time', OutputRunTime(start_time=master_pipeline.named_steps['start_timer'].start_time))\n",
    "])\n",
    "\n",
    "# Run numerical data only\n",
    "  # X = pd.DataFrame(numerical_transformer.fit_transform(cleaned_train_set[numerical_features]))\n",
    "  # X.head(10)\n",
    "\n",
    "\n",
    "##### Run on train set\n",
    "##### Last runtime = 7,094 seconds\n",
    "train_set_processed = pd.DataFrame(master_pipeline.fit_transform(X))\n",
    "validation_set_processed = pd.DataFrame(master_pipeline.transform(V))\n",
    "test_set_processed = pd.DataFrame(master_pipeline.transform(T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Fit Values to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(master_pipeline, 'master_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pipeline File & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = joblib.load('master_pipeline.joblib') \n",
    "test_set_processed = pipeline.transform(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(r'./data/1111_Preprocessed_TrainingSet.csv', index=False)\n",
    "V.to_csv(r'./data/1111_Preprocessed_ValidationSet.csv', index=False)\n",
    "y.to_csv(r'./data/1111_Preprocessed_TestSet.csv', index=False)\n",
    "# train_targets.to_csv(r'./data/Preprocessed_TrainingTargets.csv', header=['job_performance'], index=False)\n",
    "# test_targets.to_csv(r'./data/Preprocessed_TestingTargets.csv', header=['job_performance'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
